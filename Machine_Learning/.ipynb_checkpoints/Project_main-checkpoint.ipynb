{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Read in the data to a pandas DataFrame using the read_csv method.\n",
    "\n",
    "train=pd.read_excel('Normalized_relative_quantities.xlsx')\n",
    "\n",
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot two random columns to see the distribution\n",
    "plt.clf()\n",
    "sns.distplot(train['hsa-miR-1274A-002883'])\n",
    "plt.title('hsa-miR-1274A-002883')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "sns.distplot(train['hsa-miR-342-3p-002260'])\n",
    "plt.title('hsa-miR-342-3p-002260')\n",
    "plt.show()\n",
    "\n",
    "#Here we confirm that the miRNAs follow the \"normal\" negative binomial distribution for gene expression data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see again how many missing data there are / column\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we see we have 7 individuals with no prediction over if they are going to develop dm or cm (\"inc_dm_2009\", \"inc_cv_2009\"), \n",
    "#so we need to remove them from the downstream analysis \n",
    "\n",
    "train = train.dropna(how='any', subset=['inc_dm_2009', 'inc_cv_2009'])\n",
    "\n",
    "#we are also going to remove sample ids labels\n",
    "train = train.drop('CardID', 1)\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our feature varuables (all numerical) start from the 3rd column\n",
    "X = train.iloc[:,3:]\n",
    "\n",
    "#Our target variable is the \"inc_dm_2009\" or \"inc_cv_2009\" column\n",
    "y = train['inc_dm_2009']\n",
    "#we can convert it to integer \n",
    "y = y.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before any pre-processing we should split the data!\n",
    "\n",
    "#Now split the data into training and testing before pre-processing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A large amount of the total data is missing in some cases, with columns missing even 80% of the data (\"hsa-miR-25-000403\").\n",
    "\n",
    "#The chosen imputation methods did not work well with data that follow the negative bionomial distribution so as to impute and\n",
    "#log-transform and scale later --- so I will log normalise first, impute and then scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's log-transform miRNA values to follow the normal distribution\n",
    "X_log = np.log2(X)\n",
    "\n",
    "X_train_log = np.log2(X_train)\n",
    "\n",
    "X_test_log = np.log2(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation - It was found that for microarray missing values the weighted nearest neighbors imputation (knn imputation) \n",
    "#is a popular method - we will use it in columns missing 10% of the feature values - rest of the columns will be removed\n",
    "\n",
    "#remove columns with missing data > 10% in the whole dataset and then impute test and train sets:\n",
    "cols = X_log.columns[X_log.isnull().mean() < 0.1]\n",
    "\n",
    "#set sets with those columns\n",
    "X_train_log_flt = X_train_log[cols]\n",
    "X_test_log_flt = X_test_log[cols]\n",
    "\n",
    "#13/49 features were removed\n",
    "\n",
    "#imputation - I will fit and transform the train set and then fit to the test set in order to simulate real testing conditions\n",
    "from sklearn.impute import KNNImputer\n",
    "    \n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "\n",
    "X_train_imp = imputer.fit_transform(X_train_log)\n",
    "X_test_imp = imputer.transform(X_test_log) #Here we only need to transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will scale the data to [0,1] as it is necessary for some distance-based machine learning estimators (SVM, knn) \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_sc = scaler.fit_transform(X_train_imp)\n",
    "X_test_sc = scaler.transform(X_test_imp) #Here we only need to transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM model\n",
    "\n",
    "#svm is independent of the dimensionality of the feature space as the appriate selection of the regularisation parameter C \n",
    "#can prevent overfitting - so feature selection is not going to be applied here\n",
    "\n",
    "#We will do a grid search with 3 types of kernels (linear, rbf, polynomial)\n",
    "from time import process_time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "t0= process_time()\n",
    "\n",
    "parameters = [{'kernel':['linear'],'C':[0.001,0.01, 0.1, 1, 10, 100, 1000]}, \n",
    "              {'kernel':['rbf'],'C':[0.001,0.01, 0.1, 1, 10, 100, 1000], 'gamma': [0.001,0.01, 0.1, 1, 10, 100, 1000]},\n",
    "             {'kernel':['poly'],'C':[0.001,0.01, 0.1, 1, 10, 100, 1000], 'gamma': [0.001,0.01, 0.1, 1, 10, 100, 1000],\n",
    "             'degree': [1, 2, 3, 4, 5]}]\n",
    "\n",
    "svc = svm.SVC()\n",
    "svm_model = GridSearchCV(svc, parameters, cv = 5) #(Stratified)KFold is used as cross-validation strategy as \n",
    "                                                  #our target feature is binary\n",
    "svm_model.fit(X_train_sc, y_train)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "print('Best Score:', svm_model.best_score_)\n",
    "t1 = process_time() - t0\n",
    "print(\"Time elapsed: \", t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on the test data\n",
    "prediction_svm = svm_model.score(X_test_sc, y_test)\n",
    "\n",
    "print(\"Accuracy on test data:\", prediction_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of reducing the feature dimensions using PCA, we will use try the recursive feature elimination in order to check\n",
    "#if we could improve our model and discover which featuresare hughly ranked with the best hyperparameters\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "estimator = svm.SVC(kernel=, ) \n",
    "svc_knn = RFECV(estimator, step=1, cv=5)\n",
    "svc_knn.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try grid search with cleaner code\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(kernel='rbf'), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X_train_sc, y_train)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Trees\n",
    "\n",
    "#Decision trees tend to overfit on data with a large number of features so first we will perform dimensionality reduction\n",
    "#using pca\n",
    "\n",
    "#The rationale is to take advantage of the Pipeline function of sklearn in order to grid search for the best number of PCA \n",
    "#components to input in our decision trees, and the best hyperparameters of our tree at the same time \n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "decisiontree = DecisionTreeClassifier()\n",
    "\n",
    "# Create a pipeline of two steps: \n",
    "# 1) tranform the data with PCA, 2) train a Decision Tree Classifier on the data.\n",
    "pipe = Pipeline(steps=[('pca', pca), ('decisiontree', decisiontree)])\n",
    "\n",
    "# Create Parameter Space\n",
    "\n",
    "# Create a list of a sequence of integers to integrate from PCA\n",
    "n_components = list(range(1,X_train_sc.shape[1]+1,1))\n",
    "\n",
    "# Create lists of parameter for Decision Tree Classifier\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [4,6,8,12]\n",
    "\n",
    "parameters = dict(pca__n_components=n_components,\n",
    "                      decisiontree__criterion=criterion,\n",
    "                      decisiontree__max_depth=max_depth)\n",
    "\n",
    "clf = GridSearchCV(pipe, parameters, cv = 5)\n",
    "\n",
    "clf.fit(X_train_sc, y_train)\n",
    "\n",
    "print('Best Criterion:', clf.best_estimator_.get_params()['decisiontree__criterion'])\n",
    "print('Best max_depth:', clf.best_estimator_.get_params()['decisiontree__max_depth'])\n",
    "print('Best Number Of Components:', clf.best_estimator_.get_params()['pca__n_components'])\n",
    "print('Best Score:', clf.best_score_)\n",
    "\n",
    "#print(clf.best_estimator_.get_params()['decisiontree'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on the test data\n",
    "prediction_clf = clf.score(X_test_sc, y_test)\n",
    "print(\"Accuracy on test data:\", prediction_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Knn \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#define the model and parameters\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "parameters = {'n_neighbors':[1,3,5,7,9,11,13,15,17,19,21], #usually odd numbers\n",
    "              'leaf_size':[1,2,3,5],\n",
    "              'weights':['uniform', 'distance']}\n",
    "\n",
    "#Fit the model\n",
    "model_knn = GridSearchCV(knn, param_grid=parameters, cv = 5)\n",
    "model_knn.fit(X_train_sc,y_train)\n",
    "\n",
    "print('Best leaf_size:', model_knn.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best weight function:', model_knn.best_estimator_.get_params()['weights'])\n",
    "print('Best n_neighbors:', model_knn.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score:', model_knn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on the test data\n",
    "prediction_knn = model_knn.score(X_test_sc, y_test)\n",
    "print(\"Accuracy on test data:\", prediction_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe repeat knn with pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0caa4309924b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, n_jobs = -1)\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_sc' is not defined"
     ]
    }
   ],
   "source": [
    "#Random Forests\n",
    "#Let's tune the hyperparameters\n",
    "\n",
    "#Here we will try a different approach - we will use randomize grid at first to limit our search space and then we will use\n",
    "# a more exhaustive approach\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [200, 500, 1000, 1500, 2500]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [10, 20, 30, 40, 50]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15, 20]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, n_jobs = -1)\n",
    "\n",
    "rf_random.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will try:\n",
    "#SVM\n",
    "#General discriminant analysis\n",
    "#Decision Trees\n",
    "#Random Forest\n",
    "#GLMs (generalized linear models) -- logistic regression tried in paper\n",
    "#Ensembl of methods\n",
    "\n",
    "#do feature selection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection before SVM (PCA?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
