{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Read in the data to a pandas DataFrame using the read_csv method.\n",
    "\n",
    "train=pd.read_excel('Normalized_relative_quantities.xlsx')\n",
    "\n",
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot two random columns to see the distribution\n",
    "plt.clf()\n",
    "sns.distplot(train['hsa-miR-1274A-002883'])\n",
    "plt.title('hsa-miR-1274A-002883')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "sns.distplot(train['hsa-miR-342-3p-002260'])\n",
    "plt.title('hsa-miR-342-3p-002260')\n",
    "plt.show()\n",
    "\n",
    "#Here we confirm that the miRNAs follow the \"normal\" negative binomial distribution for gene expression data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inc_dm_2009                 0\n",
       "inc_cv_2009                 0\n",
       "hsa-miR-1274A-002883        0\n",
       "hsa-miR-17-002308           0\n",
       "hsa-miR-1243-002854       247\n",
       "hsa-miR-1274B-002884        1\n",
       "hsa-miR-625*-002432        47\n",
       "hsa-miR-223-002295          0\n",
       "hsa-miR-126-002228          0\n",
       "hsa-miR-484-001821          0\n",
       "hsa-miR-320-002277          0\n",
       "hsa-miR-191-002299          0\n",
       "hsa-miR-106a-002169         0\n",
       "hsa-let-7b-002619          10\n",
       "mmu-miR-451-001141         30\n",
       "hsa-miR-342-3p-002260       0\n",
       "hsa-miR-338-5P-002658       0\n",
       "hsa-let-7e-002406           3\n",
       "hsa-miR-486-001278          0\n",
       "hsa-miR-28-3p-002446      189\n",
       "hsa-miR-222-002276          1\n",
       "hsa-miR-483-5p-002338       0\n",
       "hsa-miR-122-002245        143\n",
       "hsa-miR-146b-001097         8\n",
       "hsa-miR-20b-001014         39\n",
       "hsa-miR-574-3p-002349       0\n",
       "hsa-miR-186-002285         38\n",
       "hsa-miR-145-002278         69\n",
       "hsa-miR-125a-5p-002198    371\n",
       "hsa-let-7d-002283          87\n",
       "hsa-miR-146a-000468         0\n",
       "hsa-miR-92a-000431          1\n",
       "hsa-miR-24-000402           1\n",
       "hsa-miR-150-000473          1\n",
       "hsa-miR-19b-000396          3\n",
       "hsa-miR-197-000497         30\n",
       "hsa-miR-16-000391           4\n",
       "hsa-miR-20a-000580          3\n",
       "hsa-miR-30c-000419          5\n",
       "hsa-miR-21-000397          17\n",
       "hsa-miR-30b-000602          1\n",
       "hsa-miR-26a-000405          8\n",
       "hsa-miR-142-3p-000464      20\n",
       "hsa-miR-331-000545        265\n",
       "hsa-miR-25-000403         446\n",
       "hsa-miR-335-000546        148\n",
       "hsa-miR-374-000563        285\n",
       "hsa-miR-139-5p-002289      23\n",
       "hsa-miR-720-002895          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see again how many missing data there are / column\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(553, 49)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As we see we have 7 individuals with no prediction over if they are going to develop dm or cm (\"inc_dm_2009\", \"inc_cv_2009\"), \n",
    "#so we need to remove them from the downstream analysis \n",
    "\n",
    "train = train.dropna(how='any', subset=['inc_dm_2009', 'inc_cv_2009'])\n",
    "\n",
    "#we are also going to remove sample ids labels\n",
    "train = train.drop('CardID', 1)\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our feature varuables (all numerical) start from the 3rd column\n",
    "X = train.iloc[:,3:]\n",
    "\n",
    "#Our target variable is the \"inc_dm_2009\" or \"inc_cv_2009\" column\n",
    "y = train['inc_dm_2009']\n",
    "#we can convert it to integer \n",
    "y = y.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before any pre-processing we should split the data!\n",
    "\n",
    "#Now split the data into training and testing before pre-processing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Maybe later trying to scale down the healthy subset to the levels of the two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88     0\n",
       "136    0\n",
       "99     1\n",
       "110    0\n",
       "144    1\n",
       "      ..\n",
       "62     0\n",
       "6      0\n",
       "8      1\n",
       "367    1\n",
       "285    0\n",
       "Name: inc_dm_2009, Length: 166, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A large amount of the total data is missing in some cases, with columns missing even 80% of the data (\"hsa-miR-25-000403\").\n",
    "\n",
    "#The chosen imputation methods did not work well with data that follow the negative bionomial distribution so as to impute and\n",
    "#log-transform and scale later --- so I will log normalise first, impute and then scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's log-transform miRNA values to follow the normal distribution\n",
    "X_train_log = np.log2(X_train)\n",
    "\n",
    "X_test_log = np.log2(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hsa-miR-17-002308</th>\n",
       "      <th>hsa-miR-1243-002854</th>\n",
       "      <th>hsa-miR-1274B-002884</th>\n",
       "      <th>hsa-miR-625*-002432</th>\n",
       "      <th>hsa-miR-223-002295</th>\n",
       "      <th>hsa-miR-126-002228</th>\n",
       "      <th>hsa-miR-484-001821</th>\n",
       "      <th>hsa-miR-320-002277</th>\n",
       "      <th>hsa-miR-191-002299</th>\n",
       "      <th>hsa-miR-106a-002169</th>\n",
       "      <th>...</th>\n",
       "      <th>hsa-miR-21-000397</th>\n",
       "      <th>hsa-miR-30b-000602</th>\n",
       "      <th>hsa-miR-26a-000405</th>\n",
       "      <th>hsa-miR-142-3p-000464</th>\n",
       "      <th>hsa-miR-331-000545</th>\n",
       "      <th>hsa-miR-25-000403</th>\n",
       "      <th>hsa-miR-335-000546</th>\n",
       "      <th>hsa-miR-374-000563</th>\n",
       "      <th>hsa-miR-139-5p-002289</th>\n",
       "      <th>hsa-miR-720-002895</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>-0.159000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.216002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.023000</td>\n",
       "      <td>-2.056000</td>\n",
       "      <td>-3.009001</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>0.735001</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.824999</td>\n",
       "      <td>-1.613001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.716997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.792999</td>\n",
       "      <td>1.536001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>-0.252001</td>\n",
       "      <td>-25.857000</td>\n",
       "      <td>1.498000</td>\n",
       "      <td>-1.274002</td>\n",
       "      <td>-10.100001</td>\n",
       "      <td>-0.312000</td>\n",
       "      <td>-2.650002</td>\n",
       "      <td>-2.018000</td>\n",
       "      <td>-0.725000</td>\n",
       "      <td>0.252001</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.144001</td>\n",
       "      <td>0.660999</td>\n",
       "      <td>-0.144001</td>\n",
       "      <td>-2.209000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.404001</td>\n",
       "      <td>-0.167999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>-0.025001</td>\n",
       "      <td>-26.096000</td>\n",
       "      <td>1.002999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.874001</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>-2.620002</td>\n",
       "      <td>-3.158000</td>\n",
       "      <td>0.024999</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.889001</td>\n",
       "      <td>1.336999</td>\n",
       "      <td>1.578998</td>\n",
       "      <td>0.327001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-20.016003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.766999</td>\n",
       "      <td>0.416999</td>\n",
       "      <td>0.045999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>-0.366000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.822999</td>\n",
       "      <td>-1.245002</td>\n",
       "      <td>-8.750000</td>\n",
       "      <td>-1.821000</td>\n",
       "      <td>-2.628001</td>\n",
       "      <td>-1.905000</td>\n",
       "      <td>-2.050000</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.479001</td>\n",
       "      <td>-0.759000</td>\n",
       "      <td>-2.164001</td>\n",
       "      <td>-0.884999</td>\n",
       "      <td>-2.810000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.849000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.036002</td>\n",
       "      <td>-2.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.322001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.265999</td>\n",
       "      <td>0.024998</td>\n",
       "      <td>-7.562001</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>-0.699001</td>\n",
       "      <td>0.381998</td>\n",
       "      <td>0.653997</td>\n",
       "      <td>0.322001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.847002</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>-0.597002</td>\n",
       "      <td>-0.159000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.214001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103998</td>\n",
       "      <td>0.911999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-0.006001</td>\n",
       "      <td>-15.737999</td>\n",
       "      <td>-0.968998</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>-9.881999</td>\n",
       "      <td>0.539001</td>\n",
       "      <td>-2.056000</td>\n",
       "      <td>-1.247000</td>\n",
       "      <td>0.731001</td>\n",
       "      <td>0.006001</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.104000</td>\n",
       "      <td>0.736002</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>-1.110998</td>\n",
       "      <td>-1.209999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.072998</td>\n",
       "      <td>-2.263998</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>-0.438999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>-0.104000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.535000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.346999</td>\n",
       "      <td>-2.230999</td>\n",
       "      <td>-2.475000</td>\n",
       "      <td>-1.483999</td>\n",
       "      <td>-1.205000</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.212999</td>\n",
       "      <td>-1.490000</td>\n",
       "      <td>-1.165001</td>\n",
       "      <td>-4.117998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.448999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.468000</td>\n",
       "      <td>-1.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.116000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.443000</td>\n",
       "      <td>0.188001</td>\n",
       "      <td>-7.362001</td>\n",
       "      <td>-0.266000</td>\n",
       "      <td>-0.987000</td>\n",
       "      <td>0.798999</td>\n",
       "      <td>0.084998</td>\n",
       "      <td>-0.116000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340999</td>\n",
       "      <td>0.603000</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>-1.738000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.826001</td>\n",
       "      <td>-0.661998</td>\n",
       "      <td>-0.556001</td>\n",
       "      <td>-0.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.310999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.837000</td>\n",
       "      <td>3.025000</td>\n",
       "      <td>-7.014001</td>\n",
       "      <td>2.244999</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>1.032999</td>\n",
       "      <td>0.910999</td>\n",
       "      <td>-0.310999</td>\n",
       "      <td>...</td>\n",
       "      <td>1.868999</td>\n",
       "      <td>3.070000</td>\n",
       "      <td>1.709999</td>\n",
       "      <td>1.762001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.996002</td>\n",
       "      <td>2.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>-0.148001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.219000</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>-0.484000</td>\n",
       "      <td>-0.537001</td>\n",
       "      <td>-0.801001</td>\n",
       "      <td>-0.117001</td>\n",
       "      <td>-0.383001</td>\n",
       "      <td>0.148001</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.021999</td>\n",
       "      <td>0.252001</td>\n",
       "      <td>-0.293001</td>\n",
       "      <td>-4.733999</td>\n",
       "      <td>-1.733999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.521002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.060999</td>\n",
       "      <td>-2.875999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hsa-miR-17-002308  hsa-miR-1243-002854  hsa-miR-1274B-002884  \\\n",
       "318          -0.159000                  NaN              4.216002   \n",
       "454          -0.252001           -25.857000              1.498000   \n",
       "494          -0.025001           -26.096000              1.002999   \n",
       "210          -0.366000                  NaN             -2.822999   \n",
       "38           -0.322001                  NaN              1.265999   \n",
       "..                 ...                  ...                   ...   \n",
       "72           -0.006001           -15.737999             -0.968998   \n",
       "281          -0.104000                  NaN             -2.535000   \n",
       "9             0.116000                  NaN             -0.443000   \n",
       "365           0.310999                  NaN              3.837000   \n",
       "196          -0.148001                  NaN             -2.219000   \n",
       "\n",
       "     hsa-miR-625*-002432  hsa-miR-223-002295  hsa-miR-126-002228  \\\n",
       "318                  NaN            4.023000           -2.056000   \n",
       "454            -1.274002          -10.100001           -0.312000   \n",
       "494                  NaN           -7.874001            0.162000   \n",
       "210            -1.245002           -8.750000           -1.821000   \n",
       "38              0.024998           -7.562001            0.778000   \n",
       "..                   ...                 ...                 ...   \n",
       "72              0.142000           -9.881999            0.539001   \n",
       "281                  NaN           -1.346999           -2.230999   \n",
       "9               0.188001           -7.362001           -0.266000   \n",
       "365             3.025000           -7.014001            2.244999   \n",
       "196             0.798000           -0.484000           -0.537001   \n",
       "\n",
       "     hsa-miR-484-001821  hsa-miR-320-002277  hsa-miR-191-002299  \\\n",
       "318           -3.009001            0.538000            0.735001   \n",
       "454           -2.650002           -2.018000           -0.725000   \n",
       "494           -2.620002           -3.158000            0.024999   \n",
       "210           -2.628001           -1.905000           -2.050000   \n",
       "38            -0.699001            0.381998            0.653997   \n",
       "..                  ...                 ...                 ...   \n",
       "72            -2.056000           -1.247000            0.731001   \n",
       "281           -2.475000           -1.483999           -1.205000   \n",
       "9             -0.987000            0.798999            0.084998   \n",
       "365            0.025000            1.032999            0.910999   \n",
       "196           -0.801001           -0.117001           -0.383001   \n",
       "\n",
       "     hsa-miR-106a-002169  ...  hsa-miR-21-000397  hsa-miR-30b-000602  \\\n",
       "318             0.159000  ...           0.372000            0.725000   \n",
       "454             0.252001  ...          -1.144001            0.660999   \n",
       "494             0.025001  ...          -0.889001            1.336999   \n",
       "210             0.366000  ...          -0.479001           -0.759000   \n",
       "38              0.322001  ...          -0.847002            0.976000   \n",
       "..                   ...  ...                ...                 ...   \n",
       "72              0.006001  ...          -1.104000            0.736002   \n",
       "281             0.104000  ...          -1.212999           -1.490000   \n",
       "9              -0.116000  ...           0.340999            0.603000   \n",
       "365            -0.310999  ...           1.868999            3.070000   \n",
       "196             0.148001  ...          -1.021999            0.252001   \n",
       "\n",
       "     hsa-miR-26a-000405  hsa-miR-142-3p-000464  hsa-miR-331-000545  \\\n",
       "318                 NaN              -6.824999           -1.613001   \n",
       "454           -0.144001              -2.209000                 NaN   \n",
       "494            1.578998               0.327001                 NaN   \n",
       "210           -2.164001              -0.884999           -2.810000   \n",
       "38            -0.597002              -0.159000                 NaN   \n",
       "..                  ...                    ...                 ...   \n",
       "72             0.702000              -1.110998           -1.209999   \n",
       "281           -1.165001              -4.117998                 NaN   \n",
       "9              1.110000              -1.738000                 NaN   \n",
       "365            1.709999               1.762001                 NaN   \n",
       "196           -0.293001              -4.733999           -1.733999   \n",
       "\n",
       "     hsa-miR-25-000403  hsa-miR-335-000546  hsa-miR-374-000563  \\\n",
       "318                NaN           -1.716997                 NaN   \n",
       "454                NaN                 NaN                 NaN   \n",
       "494         -20.016003                 NaN           -1.766999   \n",
       "210                NaN           -0.849000                 NaN   \n",
       "38                 NaN            0.214001                 NaN   \n",
       "..                 ...                 ...                 ...   \n",
       "72                 NaN           -0.072998           -2.263998   \n",
       "281                NaN           -3.448999                 NaN   \n",
       "9                  NaN           -0.826001           -0.661998   \n",
       "365                NaN                 NaN                 NaN   \n",
       "196                NaN            0.521002                 NaN   \n",
       "\n",
       "     hsa-miR-139-5p-002289  hsa-miR-720-002895  \n",
       "318               1.792999            1.536001  \n",
       "454              -1.404001           -0.167999  \n",
       "494               0.416999            0.045999  \n",
       "210              -3.036002           -2.484000  \n",
       "38                0.103998            0.911999  \n",
       "..                     ...                 ...  \n",
       "72                0.118000           -0.438999  \n",
       "281              -2.468000           -1.650000  \n",
       "9                -0.556001           -0.116000  \n",
       "365              -0.996002            2.007000  \n",
       "196              -1.060999           -2.875999  \n",
       "\n",
       "[387 rows x 46 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation - Different methods will be tested\n",
    "\n",
    "#First I will try two imputation methods: Univariate and Multivariate feature imputation on the entire dataset\n",
    "#Then I will remove columns with over 50% missing values and then impute with the same two methods\n",
    "\n",
    "#Univariate feature imputation using mean\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_un = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X_train_imp_1 = imp_un.fit_transform(X_train_log)\n",
    "X_test_imp_1 = imp_un.fit_transform(X_test_log)\n",
    "\n",
    "\n",
    "\n",
    "#Multivariate feature imputation\n",
    "#import numpy as np\n",
    "#from sklearn.experimental import enable_iterative_imputer\n",
    "#from sklearn.impute import IterativeImputer\n",
    "#imp_mv = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "#X_train_imp = imp_mv.fit_transform(X_train_log)\n",
    "#X_test_imp = imp_mv.fit_transform(X_test_log)\n",
    "#X_imp = imp_mv.transform(X)\n",
    "\n",
    "#X_imp\n",
    "#Remove columns with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns with missing data < 50% and then impute\n",
    "\n",
    "#find columns with < 50% missing values \n",
    "cols = X_train_log.columns[X_train_log.isnull().mean() < 0.5]\n",
    "\n",
    "#set sets with those columns\n",
    "X_train_log_flt = X_train_log[cols]\n",
    "X_test_log_flt = X_test_log[cols]\n",
    "\n",
    "\n",
    "#Univariate feature imputation using mean\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_un = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X_train_imp_2 = imp_un.fit_transform(X_train_log_flt)\n",
    "X_test_imp_2 = imp_un.fit_transform(X_test_log_flt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation using nearest neighbors imputation (knn imputation)\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "    \n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "\n",
    "X_train_imp_3 = imputer.fit_transform(X_train_log)\n",
    "X_test_imp_3 = imputer.fit_transform(X_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's try to set our numerical variables to 0 +1\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_sc = scaler.fit_transform(X_train_imp)\n",
    "X_test_sc = scaler.transform(X_test_imp) #Here we only need to fit the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7108433734939759\n"
     ]
    }
   ],
   "source": [
    "#SVM model\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(X_train_imp, y_train)\n",
    "y_pred = clf.predict(X_test_imp)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'kernel':['linear'],'C':[2**-5, 2**-3, 2**-1, 2**1, 2**3, 2**5]}, \n",
    "              {'kernel':['rbf'],'C':[2**-5, 2**-3, 2**-1, 2**1, 2**3, 2**5]}, 'gamma': ['rbf']]\n",
    "#parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv = 5)\n",
    "clf.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 0.01, 'gamma': 1e-09} with a score of 0.72\n"
     ]
    }
   ],
   "source": [
    "#Try grid search with cleaner code\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(kernel='rbf'), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X_train_sc, y_train)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 0.01, 'gamma': 1e-09} with a score of 0.72\n"
     ]
    }
   ],
   "source": [
    "#Try grid search with cleaner code\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(kernel='rbf'), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X_train_sc, y_train)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will try:\n",
    "#SVM\n",
    "#General discriminant analysis\n",
    "#Decision Trees\n",
    "#Random Forest\n",
    "#GLMs (generalized linear models) -- logistic regression tried in paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection before SVM (PCA?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first log values, then scale -1,1 or 0,1 ## one hot encoding for the categorical variables "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
