{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Read in the data to a pandas DataFrame using the read_csv method.\n",
    "\n",
    "train=pd.read_excel('Normalized_relative_quantities.xlsx')\n",
    "\n",
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot two random columns to see the distribution\n",
    "plt.clf()\n",
    "sns.distplot(train['hsa-miR-1274A-002883'])\n",
    "plt.title('hsa-miR-1274A-002883')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "sns.distplot(train['hsa-miR-342-3p-002260'])\n",
    "plt.title('hsa-miR-342-3p-002260')\n",
    "plt.show()\n",
    "\n",
    "#Here we confirm that the miRNAs follow the \"normal\" negative binomial distribution for gene expression data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see again how many missing data there are / column\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(553, 49)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As we see we have 7 individuals with no prediction over if they are going to develop dm or cm (\"inc_dm_2009\", \"inc_cv_2009\"), \n",
    "#so we need to remove them from the downstream analysis \n",
    "\n",
    "train = train.dropna(how='any', subset=['inc_dm_2009', 'inc_cv_2009'])\n",
    "\n",
    "#we are also going to remove sample ids labels\n",
    "train = train.drop('CardID', 1)\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our feature varuables (all numerical) start from the 3rd column\n",
    "X = train.iloc[:,3:]\n",
    "\n",
    "#Our target variable is the \"inc_dm_2009\" or \"inc_cv_2009\" column\n",
    "y = train['inc_dm_2009']\n",
    "#we can convert it to integer \n",
    "y = y.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before any pre-processing we should split the data!\n",
    "\n",
    "#Now split the data into training and testing before pre-processing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280    1\n",
       "293    1\n",
       "378    1\n",
       "557    0\n",
       "194    1\n",
       "      ..\n",
       "72     0\n",
       "281    1\n",
       "9      0\n",
       "365    1\n",
       "196    0\n",
       "Name: inc_dm_2009, Length: 442, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A large amount of the total data is missing in some cases, with columns missing even 80% of the data (\"hsa-miR-25-000403\").\n",
    "\n",
    "#The chosen imputation methods did not work well with data that follow the negative bionomial distribution so as to impute and\n",
    "#log-transform and scale later --- so I will log normalise first, impute and then scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's log-transform miRNA values to follow the normal distribution\n",
    "X_log = np.log2(X)\n",
    "\n",
    "X_train_log = np.log2(X_train)\n",
    "\n",
    "X_test_log = np.log2(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation - It was found that for microarray missing values the weighted nearest neighbors imputation (knn imputation) \n",
    "#is a popular method - we will use it in columns missing 10% of the feature values - rest of the columns will be removed\n",
    "\n",
    "#remove columns with missing data > 10% in the whole dataset and then impute test and train sets:\n",
    "cols = X_log.columns[X_log.isnull().mean() < 0.1]\n",
    "\n",
    "#set sets with those columns\n",
    "X_train_log_flt = X_train_log[cols]\n",
    "X_test_log_flt = X_test_log[cols]\n",
    "\n",
    "#13/49 features were removed\n",
    "\n",
    "#imputation - I will fit and transform the train set and then fit to the test set in order to simulate real testing conditions\n",
    "from sklearn.impute import KNNImputer\n",
    "    \n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "\n",
    "X_train_imp = imputer.fit_transform(X_train_log)\n",
    "X_test_imp = imputer.transform(X_test_log) #Here we only need to transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will scale the data to [0,1] as it is necessary for some distance-based machine learning estimators (SVM, knn) \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_sc = scaler.fit_transform(X_train_imp)\n",
    "X_test_sc = scaler.transform(X_test_imp) #Here we only need to transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b1bb4246e1e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m print(\"The best parameters are %s with a score of %0.2f\"\n\u001b[0;32m---> 24\u001b[0;31m       % (grid.best_params_, grid.best_score_))\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best Score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "#SVM model\n",
    "\n",
    "#svm is independent of the dimensionality of the feature space as the appriate selection of the regularisation parameter C \n",
    "#can prevent overfitting - so feature selection is not going to be applied here\n",
    "\n",
    "#We will do a grid search with 3 types of kernels (linear, rbf, polynomial)\n",
    "from time import process_time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "t0= process_time()\n",
    "\n",
    "parameters = [{'kernel':['linear'],'C':[0.001,0.01, 0.1, 1, 10, 100, 1000]}, \n",
    "              {'kernel':['rbf'],'C':[0.001,0.01, 0.1, 1, 10, 100, 1000], 'gamma': [0.001,0.01, 0.1, 1, 10, 100, 1000]},\n",
    "             {'kernel':['poly'],'C':[0.001,0.01, 0.1, 1, 10, 100, 1000], 'gamma': [0.001,0.01, 0.1, 1, 10, 100, 1000],\n",
    "             'degree': [1, 2, 3, 4, 5]}]\n",
    "\n",
    "svc = svm.SVC()\n",
    "svm_model = GridSearchCV(svc, parameters, cv = 5) #(Stratified)KFold is used as cross-validation strategy by default as \n",
    "                                                  #our target feature is binary\n",
    "svm_model.fit(X_train_sc, y_train)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (svm_model.best_params_, svm_model.best_score_))\n",
    "\n",
    "t1 = process_time() - t0\n",
    "print(\"Time elapsed: \", t1) #takes 40min to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 0.001, 'kernel': 'linear'} with a score of 0.72\n",
      "Time elapsed:  2485.96875\n"
     ]
    }
   ],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (svm_model.best_params_, svm_model.best_score_))\n",
    "\n",
    "t1 = process_time() - t0\n",
    "print(\"Time elapsed: \", t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.71\n"
     ]
    }
   ],
   "source": [
    "#predictions on the test data\n",
    "prediction_svm = svm_model.score(X_test_sc, y_test)\n",
    "\n",
    "print(\"Accuracy on test data: %0.2f\" % prediction_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features : 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Instead of reducing the feature dimensions using PCA, we will use try the recursive feature elimination in order to check\n",
    "#if we could improve our model and discover which featuresare hughly ranked with the best hyperparameters\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "estimator = svm.SVC(kernel='linear', C=0.001) \n",
    "svc_rfe = RFECV(estimator, step=1, cv=5)\n",
    "svc_rfe.fit(X_train_sc, y_train)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % svc_rfe.n_features_)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(svc_rfe.grid_scores_) + 1), svc_rfe.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see that recursive feature elimination does not improve our score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Trees\n",
    "\n",
    "#Decision trees tend to overfit on data with a large number of features so first we will perform dimensionality reduction\n",
    "#using pca\n",
    "\n",
    "#The rationale is to take advantage of the Pipeline function of sklearn in order to grid search for the best number of PCA \n",
    "#components to input in our decision trees, and the best hyperparameters of our tree at the same time \n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "decisiontree = DecisionTreeClassifier()\n",
    "\n",
    "# Create a pipeline of two steps: \n",
    "# 1) tranform the data with PCA, 2) train a Decision Tree Classifier on the data.\n",
    "pipe = Pipeline(steps=[('pca', pca), ('decisiontree', decisiontree)])\n",
    "\n",
    "# Create Parameter Space\n",
    "\n",
    "# Create a list of a sequence of integers to integrate from PCA\n",
    "n_components = list(range(1,X_train_sc.shape[1]+1,1))\n",
    "\n",
    "# Create lists of parameter for Decision Tree Classifier\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [4,6,8,12]\n",
    "\n",
    "parameters = dict(pca__n_components=n_components,\n",
    "                      decisiontree__criterion=criterion,\n",
    "                      decisiontree__max_depth=max_depth)\n",
    "\n",
    "clf = GridSearchCV(pipe, parameters, cv = 5)\n",
    "\n",
    "clf.fit(X_train_sc, y_train)\n",
    "\n",
    "print('Best Criterion:', clf.best_estimator_.get_params()['decisiontree__criterion'])\n",
    "print('Best max_depth:', clf.best_estimator_.get_params()['decisiontree__max_depth'])\n",
    "print('Best Number Of Components:', clf.best_estimator_.get_params()['pca__n_components'])\n",
    "print(\"Best Score: %0.2f\" % clf.best_score_)\n",
    "\n",
    "#print(clf.best_estimator_.get_params()['decisiontree'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on the test data\n",
    "prediction_clf = clf.score(X_test_sc, y_test)\n",
    "print(\"Accuracy on test data:\", prediction_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Knn \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#define the model and parameters\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "parameters = {'n_neighbors':[1,3,5,7,9,11,13,15,17,19,21], #usually odd numbers\n",
    "              'leaf_size':[1,2,3,5],\n",
    "              'weights':['uniform', 'distance']}\n",
    "\n",
    "#Fit the model\n",
    "model_knn = GridSearchCV(knn, param_grid=parameters, cv = 5)\n",
    "model_knn.fit(X_train_sc,y_train)\n",
    "\n",
    "print('Best leaf_size:', model_knn.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best weight function:', model_knn.best_estimator_.get_params()['weights'])\n",
    "print('Best n_neighbors:', model_knn.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score:', model_knn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on the test data\n",
    "prediction_knn = model_knn.score(X_test_sc, y_test)\n",
    "print(\"Accuracy on test data:\", prediction_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe repeat knn with pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs...\n",
       "                   iid='deprecated', n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50],\n",
       "                                        'max_features': ['auto', 'log2'],\n",
       "                                        'min_samples_leaf': [1, 2, 5, 10],\n",
       "                                        'min_samples_split': [2, 5, 10, 15, 20],\n",
       "                                        'n_estimators': [200, 500, 1000, 1500,\n",
       "                                                         2500]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forests\n",
    "#Let's tune the hyperparameters\n",
    "\n",
    "#Here we will try a different approach - we will use randomize grid at first to limit our search space and then we will use\n",
    "# a more exhaustive approach\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [200, 500, 1000, 1500, 2500]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [10, 20, 30, 40, 50]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15, 20]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, n_jobs = -1)\n",
    "\n",
    "rf_random.fit(X_train_sc, y_train)\n",
    "\n",
    "print(\"Best Score:\", rf_random.best_params_)\n",
    "print(\"Best Score: %0.2f\" % rf_random.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: {'bootstrap': True, 'max_depth': 50, 'max_features': 'auto', 'min_samples_leaf': 8, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "Best Score: 0.73\n"
     ]
    }
   ],
   "source": [
    "#Random Forests 2\n",
    "#Let's see if we can improve the score by grinding search around the best parameters\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#define the model and parameters\n",
    "rf_v2 = RandomForestClassifier()\n",
    "\n",
    "n_estimators = [300, 500, 800, 1000]\n",
    "max_features = ['auto']\n",
    "max_depth = [30, 40, 50, 60]\n",
    "min_samples_split = [10, 15, 20, 25]\n",
    "min_samples_leaf = [6, 8, 10, 12]\n",
    "bootstrap = [True]\n",
    "\n",
    "parameters = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "#Fit the model\n",
    "model_rf_v2 = GridSearchCV(rf_v2, param_grid=parameters, cv = 5)\n",
    "model_rf_v2.fit(X_train_sc,y_train)\n",
    "\n",
    "print(\"Best Score:\", model_rf_v2.best_params_)\n",
    "print(\"Best Score: %0.2f\" % model_rf_v2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: {'bootstrap': True, 'max_depth': 50, 'max_features': 'auto', 'min_samples_leaf': 8, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "Best Score: 0.73\n"
     ]
    }
   ],
   "source": [
    "#The score did not change whatsoever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will try:\n",
    "#SVM\n",
    "#General discriminant analysis\n",
    "#Decision Trees\n",
    "#Random Forest\n",
    "#GLMs (generalized linear models) -- logistic regression tried in paper\n",
    "#Ensembl of methods\n",
    "#linear model elastic net\n",
    "\n",
    "#do feature selection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection before SVM (PCA?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
