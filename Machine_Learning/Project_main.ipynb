{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Read in the data to a pandas DataFrame using the read_csv method.\n",
    "\n",
    "train=pd.read_excel('Normalized_relative_quantities.xlsx')\n",
    "\n",
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot two random columns to see the distribution\n",
    "plt.clf()\n",
    "sns.distplot(train['hsa-miR-1274A-002883'])\n",
    "plt.title('hsa-miR-1274A-002883')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "sns.distplot(train['hsa-miR-342-3p-002260'])\n",
    "plt.title('hsa-miR-342-3p-002260')\n",
    "plt.show()\n",
    "\n",
    "#Here we confirm that the miRNAs follow the \"normal\" negative binomial distribution for gene expression data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see again how many missing data there are / column\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(553, 49)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As we see we have 7 individuals with no prediction over if they are going to develop dm or cm (\"inc_dm_2009\", \"inc_cv_2009\"), \n",
    "#so we need to remove them from the downstream analysis \n",
    "\n",
    "train = train.dropna(how='any', subset=['inc_dm_2009', 'inc_cv_2009'])\n",
    "\n",
    "#we are also going to remove sample ids labels\n",
    "train = train.drop('CardID', 1)\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 3,
>>>>>>> ab03c0290540fcfd23a6f028c31adde1f310405f
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    398\n",
       "1.0    155\n",
       "Name: inc_dm_2009, dtype: int64"
      ]
     },
<<<<<<< HEAD
     "execution_count": 15,
=======
     "execution_count": 3,
>>>>>>> ab03c0290540fcfd23a6f028c31adde1f310405f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unbalanced dataset - as it is\n",
    "\n",
    "#Our feature varuables (all numerical) start from the 3rd column\n",
    "X = train.iloc[:,3:]\n",
    "\n",
    "#Our target variable is the \"inc_dm_2009\" or \"inc_cv_2009\" column\n",
    "y = train['inc_dm_2009']\n",
    "#we can convert it to integer \n",
    "y = y.astype(int)\n",
    "\n",
    "#See if there is imbalance of the target value\n",
    "train['inc_dm_2009'].value_counts()\n",
    "\n",
    "#Yes there is - 1 cases 2.5 times more - so we will try a stratified split"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced dataset - Up-sample Minority Class\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = train[train['inc_dm_2009']==0].drop(['inc_cv_2009'], axis=1)\n",
    "df_minority = train[train['inc_dm_2009']==1].drop(['inc_cv_2009'], axis=1)\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=398,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "\n",
    "X = df_upsampled.iloc[:,3:]\n",
    "y = train['inc_dm_2009']\n",
    "\n",
    "# Display new class counts\n",
    "df_upsampled['inc_dm_2009'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced dataset - Down-sample Majority Class\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = train[train['inc_dm_2009']==0].drop(['inc_cv_2009'], axis=1)\n",
    "df_minority = train[train['inc_dm_2009']==1].drop(['inc_cv_2009'], axis=1)\n",
    "\n",
    "# Upsample minority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,     # sample with replacement\n",
    "                                 n_samples=155,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Display new class counts\n",
    "#df_upsampled['inc_dm_2009'].value_counts()\n",
    "\n",
    "X = df_downsampled.iloc[:,3:]\n",
    "y = train['inc_dm_2009']\n",
    "\n",
    "# Display new class counts\n",
    "df_downsampled['inc_dm_2009'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
=======
   "execution_count": 21,
>>>>>>> ab03c0290540fcfd23a6f028c31adde1f310405f
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hsa-miR-17-002308', 'hsa-miR-1274B-002884', 'hsa-miR-625*-002432',\n",
      "       'hsa-miR-223-002295', 'hsa-miR-126-002228', 'hsa-miR-484-001821',\n",
      "       'hsa-miR-320-002277', 'hsa-miR-191-002299', 'hsa-miR-106a-002169',\n",
      "       'hsa-let-7b-002619', 'mmu-miR-451-001141', 'hsa-miR-342-3p-002260',\n",
      "       'hsa-miR-338-5P-002658', 'hsa-let-7e-002406', 'hsa-miR-486-001278',\n",
      "       'hsa-miR-222-002276', 'hsa-miR-483-5p-002338', 'hsa-miR-146b-001097',\n",
      "       'hsa-miR-20b-001014', 'hsa-miR-574-3p-002349', 'hsa-miR-186-002285',\n",
      "       'hsa-miR-146a-000468', 'hsa-miR-92a-000431', 'hsa-miR-24-000402',\n",
      "       'hsa-miR-150-000473', 'hsa-miR-19b-000396', 'hsa-miR-197-000497',\n",
      "       'hsa-miR-16-000391', 'hsa-miR-20a-000580', 'hsa-miR-30c-000419',\n",
      "       'hsa-miR-21-000397', 'hsa-miR-30b-000602', 'hsa-miR-26a-000405',\n",
      "       'hsa-miR-142-3p-000464', 'hsa-miR-139-5p-002289', 'hsa-miR-720-002895'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Unbalanced - splitting after process\n",
    "### - Let's test a little cheating\n",
    "#First we will process all the dataset and then we will split our training and testing sets\n",
    "\n",
    "X_log = np.log2(X)\n",
    "\n",
    "#Imputation\n",
    "\n",
    "#remove columns with missing data > 10% in the whole dataset and impute:\n",
    "cols = X_log.columns[X_log.isnull().mean() < 0.1]\n",
    "print(cols)\n",
    "#set sets with those columns\n",
    "X_log_flt = X_log[cols]\n",
    "\n",
    "#13/49 features were removed\n",
    "\n",
    "# will fit and transform with KNN imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "    \n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "\n",
    "X_imp = imputer.fit_transform(X_log_flt)\n",
    "\n",
<<<<<<< HEAD
    "X_imp = X_log_flt\n",
    "\n",
=======
    "#X_imp = X_log_flt\n",
>>>>>>> ab03c0290540fcfd23a6f028c31adde1f310405f
    "#Scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_sc = scaler.fit_transform(X_log_flt)\n",
    "\n",
    "#Split the set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_sc, X_test_sc, y_train, y_test = train_test_split(X_sc, y, test_size=0.2, random_state=0, stratify = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 6,
>>>>>>> ab03c0290540fcfd23a6f028c31adde1f310405f
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unbalanced - splitting before process\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state=42)\n",
    "\n",
    "#A large amount of the total data is missing in some cases, with columns missing even 80% of the data (\"hsa-miR-25-000403\").\n",
    "\n",
    "#The chosen imputation methods did not work well with data that follow the negative bionomial distribution so as to impute and\n",
    "#log-transform and scale later --- so I will log normalise first, impute and then scale\n",
<<<<<<< HEAD
    "\n",
=======
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> ab03c0290540fcfd23a6f028c31adde1f310405f
    "#Let's log-transform miRNA values to follow the normal distribution\n",
    "X_log = np.log2(X)\n",
    "\n",
    "X_train_log = np.log2(X_train)\n",
    "\n",
<<<<<<< HEAD
    "X_test_log = np.log2(X_test)\n",
    "\n",
=======
    "X_test_log = np.log2(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> ab03c0290540fcfd23a6f028c31adde1f310405f
    "#Imputation - It was found that for microarray missing values the weighted nearest neighbors imputation (knn imputation) \n",
    "#is a popular method - we will use it in columns missing 10% of the feature values - rest of the columns will be removed\n",
    "\n",
    "#remove columns with missing data > 10% in the whole dataset and then impute test and train sets:\n",
    "cols = X_log.columns[X_log.isnull().mean() < 0.1]\n",
    "\n",
    "#set sets with those columns\n",
    "X_train_log_flt = X_train_log[cols]\n",
    "X_test_log_flt = X_test_log[cols]\n",
    "\n",
    "#13/49 features were removed\n",
    "\n",
    "#imputation - I will fit and transform the train set and then fit to the test set in order to simulate real testing conditions\n",
    "from sklearn.impute import KNNImputer\n",
    "    \n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "\n",
    "X_train_imp = imputer.fit_transform(X_train_log_flt)\n",
<<<<<<< HEAD
    "X_test_imp = imputer.transform(X_test_log_flt) #Here we only need to transform the test data\n",
    "\n",
=======
    "X_test_imp = imputer.transform(X_test_log_flt) #Here we only need to transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> ab03c0290540fcfd23a6f028c31adde1f310405f
    "#Now we will scale the data to [0,1] as it is necessary for some distance-based machine learning estimators (SVM, knn) \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_sc = scaler.fit_transform(X_train_imp)\n",
    "X_test_sc = scaler.transform(X_test_imp) #Here we only need to transform the test data\n",
    "\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our dataset is imbalanced with more negative than positive target values (2.5 times more) since diabetic patients are fewer\n",
    "#than healthy patients in all study cohorts. That means that we would expect more false negatives \n",
    "#(diabetic identified as healthy) than false positives (healthy identified as diabetic), so F1-score \n",
    "#(the balance between Precision and Recall) is the most appropriate metric and the most interpretable one.\n",
    "#On the other hand, a ROC curve is more insensitive to imbalancing data as it averages many F1-score values across a spectrum\n",
    "#of different thresholds."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 22,
>>>>>>> ab03c0290540fcfd23a6f028c31adde1f310405f
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "The best parameters are {'C': 1, 'gamma': 100, 'kernel': 'rbf'} with a score of 0.72\n",
      "Time elapsed:  13.40625\n",
=======
      "The best parameters are {'C': 100, 'gamma': 1, 'kernel': 'rbf'} with a score of 0.51\n",
      "Time elapsed:  57.234375\n",
>>>>>>> ab03c0290540fcfd23a6f028c31adde1f310405f
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.80      0.77        80\n",
      "           1       0.36      0.29      0.32        31\n",
      "\n",
      "    accuracy                           0.66       111\n",
      "   macro avg       0.55      0.55      0.55       111\n",
      "weighted avg       0.64      0.66      0.65       111\n",
      "\n",
      "Confusion Matrix: \n",
      " [[64 16]\n",
      " [22  9]]\n"
     ]
    }
   ],
   "source": [
    "#SVM model\n",
    "\n",
    "#svm is independent of the dimensionality of the feature space as the appriate selection of the regularisation parameter C \n",
    "#can prevent overfitting - so feature selection is not going to be applied here\n",
    "\n",
    "#We will do a grid search with 3 types of kernels (linear, rbf, polynomial)\n",
    "from time import process_time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "t0= process_time()\n",
    "\n",
    "parameters = [{'kernel':['linear'],'C':[0.001,0.01, 0.1, 1, 10, 100, 1000]}, \n",
    "              {'kernel':['rbf'],'C':[0.001,0.01, 0.1, 1, 10, 100, 1000], 'gamma': [0.001,0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "\n",
<<<<<<< HEAD
    "svc = svm.SVC(class_weight='balanced')\n",
    "svm_model = GridSearchCV(svc, parameters, cv = 5) #(Stratified)KFold is used as cross-validation strategy by default as \n",
=======
    "svc = svm.SVC()\n",
    "svm_model = GridSearchCV(svc, parameters, cv = 10, scoring = 'f1_macro') \n",
    "#(Stratified)KFold is used as cross-validation strategy by default as \n",
>>>>>>> ab03c0290540fcfd23a6f028c31adde1f310405f
    "                                                  #our target feature is binary\n",
    "\n",
    "svm_model = svm_model.fit(X_train_sc, y_train) \n",
    "   \n",
    "  \n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (svm_model.best_params_, svm_model.best_score_))\n",
    "\n",
    "t1 = process_time() - t0\n",
    "print(\"Time elapsed: \", t1) #takes 40min to run\n",
    "\n",
    "\n",
    "\n",
    "y_true, y_pred = y_test, svm_model.predict(X_test_sc)\n",
    "print(\"Classification Report:\", \"\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\", \"\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Kernel: rbf\n",
      "Best C: 100\n",
      "Best gamma: 10\n",
      "Best Number Of Components: 4\n",
      "Best Score: 0.57\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68        80\n",
      "           1       0.19      0.19      0.19        31\n",
      "\n",
      "    accuracy                           0.54       111\n",
      "   macro avg       0.44      0.43      0.43       111\n",
      "weighted avg       0.55      0.54      0.54       111\n",
      "\n",
      "Confusion Matrix: \n",
      " [[54 26]\n",
      " [25  6]]\n"
     ]
    }
   ],
   "source": [
    "#Here we see that our model cannot predict any positive result. Maybe there is noise in the features?\n",
    "\n",
    "\n",
    "#Let's reduce the feature dimensions using PCA and grid search again - I will do it for linear and rbf kernel seperately\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import svm\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Create a pipeline of two steps: \n",
    "# 1) tranform the data with PCA, 2) train a Decision Tree Classifier on the data.\n",
    "pipe = Pipeline(steps=[('pca', pca), ('svc', svc)])\n",
    "\n",
    "# Create Parameter Space\n",
    "\n",
    "# Create a list of a sequence of integers to integrate from PCA\n",
    "n_components = list(range(1,X_train_sc.shape[1]+1,1))\n",
    "\n",
    "# Create lists of parameter for SVC\n",
    "#kernel = ['linear', 'rbf']\n",
    "#kernel = ['linear']\n",
    "C = [0.001,0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "kernel = ['rbf']\n",
    "gamma = [0.001,0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "#parameters = dict(pca__n_components=n_components,\n",
    "#                      svc__kernel=kernel,\n",
    "#                      svc__C=C)\n",
    "\n",
    "parameters = dict(pca__n_components=n_components,\n",
    "                      svc__kernel=kernel,\n",
    "                      svc__C=C, svc__gamma=gamma)\n",
    "\n",
    "svm_model = GridSearchCV(pipe, parameters, cv = 10, scoring = 'f1_macro')\n",
    "\n",
    "svm_model.fit(X_train_sc, y_train)\n",
    "\n",
    "print('Best Kernel:', svm_model.best_estimator_.get_params()['svc__kernel'])\n",
    "print('Best C:', svm_model.best_estimator_.get_params()['svc__C'])\n",
    "print('Best gamma:', svm_model.best_estimator_.get_params()['svc__gamma'])\n",
    "print('Best Number Of Components:', svm_model.best_estimator_.get_params()['pca__n_components'])\n",
    "print(\"Best Score: %0.2f\" % svm_model.best_score_)\n",
    "print()\n",
    "\n",
    "y_true, y_pred = y_test, svm_model.predict(X_test_sc)\n",
    "print(\"Classification Report:\", \"\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\", \"\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features : 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEbCAYAAADAsRPLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de7wcZZ3n8c+XAKJAUAy4GMAERF10EZ3DReHlgKMMRjSiXIwyXmAHI7coIwsz44i46yoqDDPe45VBhEVQRASC4wA64EAS7iEyhgASYAiIQLgoJHz3j6qG5tjn9JNwKtU5+b5fr351V3VV9/cU4fxO1fPU88g2ERERw63TdoCIiBhMKRAREdFTCkRERPSUAhERET2lQERERE8pEBER0VMKRERE9LRuyUaSNgd2A14MPAbcCMyz/WSD2SIiokUa7UY5SXsCxwGbAtcAS4ENgJcB2wJnAyfZfqj5qBERsTr1KxCfB75o+7c93lsX2AeYYPuc5iJGREQbRi0QERGx9ipqpJY0S9JEVb4l6WpJezUdLiIi2lPai+ngup1hL2Az4IPAZxtLFRERrSstEKqfpwHfsX1d17qIiBiHSgvEfEkXUxWIOZI2BtLFNSJiHCtqpJa0DrAjsNj2A5JeCEy2fX3TASMioh1FN8rZflLSPcD2dffWiIgY50rvpD4ROBC4CVhRrzbwi4ZyRUREy0ovMd0M7GD7j81HioiIQVDaSL0YWK/JIBERMVhK2xMeBa6V9HPgqbMI20c1kioiIlpXWiDOqx8REbGWKB6LSdL6VKO4Atxs+4nGUkVEROtKG6n3AE4FbqO6g3or4P2204spImKcKi0Q84H32L65Xn4ZcIbtP2s4X0REtKS0F9N6neIAYPs/Sa+miIhxrbSRep6kbwGn1cvvBeY3E2nVTZo0yVOmTGk7RkTEGmP+/Pn32d6s13ulBeLDwOHAUVRtEL8AvjI28cbOlClTmDdvXtsxIiLWGJJuH+m90rGY/gicXD8iImItMGobhKSz6ucbJF0//NHvwyXtLelmSYskHTfKdjtJWiFpv2HrJ0i6RtL5pT9QRESMjX5nELPq531W9oMlTQC+DLwZWALMlXSe7Zt6bHciMGeE718ITFzZ74+IiGdn1DMI23fXLw+zfXv3Azisz2fvDCyyvdj248CZwPQe2x0JnAMs7V4paUvgrcA3C36OiIgYY6XdXN/cY91b+uwzGbija3lJve4pkiYD+wJf67H/KcD/IjPXRUS0ol8bxIcl3QC8fFj7w61AvzaIXnNWD78r7xTgWNsruldK2gdYartvV1pJh0qaJ2nevffe22/ziIgo1K8N4vvAhcBngO5G5mW27++z7xKqITk6tgTuGrbNEHCmJIBJwDRJy4FdgLdLmgZsAEyU9D3bBw3/EtuzgdkAQ0NDZQNLRUREX6MWCNsPAg8CMwAkbU71C3sjSRvZ/u0ou88FtpM0FbgTeDfwnmGfP7XzWtJ3gfNtnwucC/xtvX4P4GO9ikNERDSnqA1C0tsk/Qa4FbiMatC+C0fbx/Zy4Aiq3kkLgbNsL5A0U9LMZ5U6IiIaVzpY33XAG4F/tf0aSXsCM2wf2nTAlTE0NOTcSR0RUU7SfNtDvd4r7cX0hO3fAetIWsf2JcCOY5YwIiIGTulYTA9I2ohqDKbTJS0FljcXKyIi2lZ6BjGdal7qjwIXAbcAb2sqVEREtK/0DGJz4G7bfwBOlfRc4EXA7xpLFhERrSo9g/gBz7yjeUW9LiIixqnSArFuPZ4SAPXr9ZuJFBERg6C0QNwr6e2dBUnTgfuaiRQREYOgtA1iJlXvpS9RjbF0B/C+xlJFRETrSmeUuwXYte7qKtvLmo0VERFtG7VASDrI9vckHT1sPQC2MwVpRMQ41e8M4nn188ZNB4mIiMHSr0BsWz/fZDvdWiMi1iL9ejFNk7Qe9dDbERGx9uh3BnERVXfWDSU91LVegG1PbCxZRES0atQzCNvH2N4E+KntiV2PjVMcIiLGt6Ib5WxPbzpIREQMllELhKR/r5+XSXqofu48Hhpt34iIWLP1m5N69/o53VwjItYypXNSbyvpOfXrPSQdJen5zUaLiIg2lQ7Wdw6wQtJLgW8BU4HvN5YqIiJaV1ognrS9HNgXOMX2R4EtmosVERFtKy0QT0iaAbwfOL9et14zkSIiYhCUFogPAq8DPm37VklTge81FysiItpWOtz3TcBRAJJeAGxs+7NNBouIiHaV9mK6VNJESZsC1wHfkZShviMixrHSS0yb2H4IeCfwHdt/BrypuVgREdG20gKxrqQtgAN4upE6IiLGsdIC8SlgDrDI9lxJ2wC/aS5WRES0rbSR+gfAD7qWFwPvaipURES0r6hASNoAOAR4JbBBZ73tgxvKFRERLSu9xHQa8N+AvwQuA7YEljUVKiIi2ldaIF5q+x+AR2yfCrwV+B/NxYqIiLYVD7VRPz8g6VXAJsCURhJFRMRAKGqDAGbXd1D/A3AesBHwicZSRURE60p7MX2zfnkZsE1zcSIiYlCMWiAkHT3a+7Yz3EZExDjV7wwiU41GRKyl+s1JfcLqChIREYOldDTXU7vnoJb0Aknfbi5WRES0rbSb6w62H+gs2P498Jp+O0naW9LNkhZJOm6U7XaStELSfvXyBpKuknSdpAWSciYTEbGalRaIdepurgDU80L0a+CeAHwZeAuwPTBD0vYjbHci1WCAHX8E3mj71cCOwN6Sdi3MGhERY6D0PoiTgCsknQ2YatjvT/fZZ2eq0V8XA0g6E5gO3DRsuyOBc4CdOitsG3i4Xlyvfrgwa0REjIGiMwjb/0I1eus9wL3AO22f1me3ycAdXctL6nVPkTQZ2Bf42vCdJU2QdC2wFPiZ7St7fYmkQyXNkzTv3nvvLflxIiKiQOkZRGde6uF//Y9GvT5m2PIpwLG2V0jP3Nz2CmDHunH8R5JeZfvGHrlmA7MBhoaGcpYRETFGigvEKlgCbNW1vCVw17BthoAz6+IwCZgmabntczsb2H5A0qXA3sCfFIiIiGhGaSP1qpgLbCdpqqT1gXdTjeP0FNtTbU+xPQU4GzjM9rmSNut0q5X0XKr5r3/dYNaIiBim9D6IE0vWdbO9HDiCqnfSQuAs2wskzZQ0s89XbgFcIul6qkLzM9uZCzsiYjVS1WGoz0bS1bZfO2zd9bZ3aCzZKhgaGvK8efPajhERscaQNN/2UK/3+t3L8GHgMGDb+q/5jo2BK8YuYkREDJp+jdTfBy4EPgN03wm9zPb9jaWKiIjWjdoGYftB27cB/wTcb/t227cDT0jaZXUEjIiIdpT2YvoqT9/ZDPBIvS4iIsap0gIhd7Vm236SZu+hiIiIlpUWiMWSjpK0Xv2YBSxuMlhERLSrtEDMBF4P3El1h/QuwKFNhYqIiPYVXSayvZTqTuiIiFhLlN5J/TJJP5d0Y728g6SPNxstIiLaVHqJ6RvA3wJPANi+npxRRESMa6UF4nm2rxq2bvlYh4mIiMFRWiDuk7Qt9XwO9dzRdzeWKiIiWld6L8PhVJPyvELSncCtwHsbSxUREa3rWyAkrQMM2X6TpA2BdWwvaz5aRES0qe8lpvqu6SPq14+kOERErB1K2yB+JuljkraStGnn0WiyiIhoVWkbxMH18+Fd6wxsM7ZxIiJiUJS2QRxk+/LVkCciIgZEaRvEF1ZDloiIGCClbRAXS3qXJDWaJiIiBkZpG8TRwIbACkmPAQJse2JjySIiolWlo7lu3HSQiIgYLMWzwkl6O/CGevFS2+c3EykiIgZB6XDfnwVmATfVj1n1uoiIGKdKzyCmATvWPZqQdCpwDXBcU8EiIqJdpb2YAJ7f9XqTsQ4SERGDpfQM4jPANZIuoerB9AaqCYQiImKcKu3FdIakS4GdqArEsbb/q8lgERHRrtJG6n2BR22fZ/vHwB8kvaPZaBER0abSNojjbT/YWbD9AHB8M5EiImIQlBaIXtsV30MRERFrntICMU/SyZK2lbSNpH8E5jcZLCIi2lVaII4EHgf+H3AW8BjPnBsiIiLGmdJeTI+Qm+IiItYqK3OjXERErEVSICIioqdRC4SkE+vn/VdPnIiIGBT9ziCmSVqPDKsREbHW6VcgLgLuA3aQ9JCkZd3P/T5c0t6Sbpa0SNKIjdySdpK0QtJ+9fJWki6RtFDSAkmzVuqnioiIZ23UAmH7GNubAD+1PdH2xt3Po+0raQLwZeAtwPbADEnbj7DdicCcrtXLgb+x/d+BXYHDe+0bERHNKWqktj1d0osk7VM/NivYbWdgke3Fth8HzgSm99juSOAcYGnX991t++r69TJgITC5JGtERIyN0sH69geuAvYHDgCu6lwOGsVk4I6u5SUM+yUvaTKwL/C1Ub57CvAa4MqSrBERMTZKx1P6OLCT7aUA9RnEvwJnj7KPeqzzsOVTqIYOXyH96eaSNqI6u/iI7Z5tHpIOBQ4F2Hrrrfv8GBERUaq0QKzTKQ6139H/7GMJsFXX8pbAXcO2GQLOrIvDJKpeU8ttn1v3njoHON32D0f6EtuzgdkAQ0NDwwtQRESsotICcZGkOcAZ9fKBwAV99pkLbCdpKnAn8G7gPd0b2J7aeS3pu8D5dXEQ8C1goe2TCzNGRMQYKh2L6RhJ7wR2p7p0NNv2j/rss1zSEVS9kyYA37a9QNLM+v0R2x2A3YC/Am6QdG297u9s9ytKERExRmSPn6syQ0NDnjdvXtsxIiLWGJLm2x7q9V7GYoqIiJ4yKxxwwk8WcNNdfW8Mj4gYSNu/eCLHv+2VY/65xQVC0vrAK6i6qt5c3/wWERHjVFGBkPRWqpvZbqFqpJ4q6UO2L2wy3OrSROWNiFjTlZ5BnATsaXsRgKRtgZ8C46JARETEnyptpF7aKQ61xXSNnRQREePPqGcQ9b0PAAskXQCcRdUGsT/VjXARETFO9bvE9Lau1/cAf16/vhd4QSOJIiJiIIxaIGx/cHUFiYiIwVLai2kz4K+BKd372D64mVgREdG20l5MPwZ+STXE94rm4kRExKAoLRDPs31so0kiImKglHZzPV/StEaTRETEQCktELOoisRjkh6StExSBi+KiBjHSueD2LjpIBERMVhGPYOQNKXP+5K05VgGioiIwdDvDOLzktah6sU0n+oGuQ2AlwJ7An8BHE81/3RERIwj/W6U21/S9sB7gYOBLYBHgYVUc1J/2vYfGk8ZERGrXd82CNs3AX+/GrJERMQAyZSjERHRUwpERET0lAIRERE9FRWIujvrQZI+US9vLWnnZqNFRESbSs8gvgK8DphRLy8DvtxIooiIGAilg/XtYvu1kq4BsP17Ses3mCsiIlpWegbxhKQJVNONduaHeLKxVBER0brSAvHPwI+AzSV9Gvh34P82lioiIlpXOljf6ZLmUw2tIeAdthc2miwiIlrVt0DUYzFdb/tVwK+bjxQREYOg7yUm208C10naejXkiYiIAVHai2kLYIGkq4BHOittv72RVBER0brSAnFCoykiImLglDZSXybpRcBO9aqrbC9tLlZERLStdKiNA4CrgP2BA4ArJe3XZLCIiGhX6SWmvwd26pw11DfK/StwdlPBIiKiXaU3yq0z7JLS71Zi34iIWAOVnkFcJGkOcEa9fCBwYTORIiJiEJQ2Uh8j6Z3A7lR3Us+2/aNGk0VERKtKG6mnAhfYPtr2R6nOKKYU7Le3pJslLZJ03Cjb7SRpRXfDt6RvS1oq6caSjBERMbZK2xF+wDNHb11RrxtRPfrrl4G3ANsDMyRtP8J2JwJzhr31XWDvwnwRETHGSgvEurYf7yzUr/vNB7EzsMj24nr7M4HpPbY7EjgHeMZ9FbZ/AdxfmC8iIsZYaYG4V9JTw2pImg7c12efycAdXctL6nVPkTQZ2Bf4WmGOiIhYTUp7Mc0ETpf0JapG6juA9/XZRz3WedjyKcCxtldIvTbvT9KhwKEAW2+d8QQjIsZKaS+mW4BdJW0EyPaygt2WAFt1LW8J3DVsmyHgzLo4TAKmSVpu+9ySXHW22cBsgKGhoeEFKCIiVlFpL6ZZkiZSjeT6j5KulrRXn93mAttJmlrPX/1u4LzuDWxPtT3F9hSqu7IPW5niEBERzSltgzjY9kPAXsDmwAeBz462g+3lwBFUvZMWAmfZXiBppqSZ/b5Q0hnAr4CXS1oi6ZDCrBERMQZK2yA6DQTTgO/Yvk4FjQa2LwAuGLauZ4O07Q8MW55RmC0iIhpQegYxX9LFVAVijqSNeeZ9ERERMc6UnkEcAuwILLb9qKQXUl1mioiIcaq0F9OTwNVdy7+jGtE1IiLGqQzZHRERPaVARERET6VtEJ1B9V7UvY/t3zYRKiIi2ldUICQdCRwP3MPTvZcM7NBQroiIaFnpGcQs4OV143RERKwFStsg7gAebDJIREQMltIziMXApZJ+Cvyxs9L2yY2kioiI1pUWiN/Wj/XpP1FQRESMA6U3yp0AUA+xYdsPN5oqIiJaVzrc96skXQPcCCyQNF/SK5uNFhERbSptpJ4NHG37JbZfAvwN8I3mYkVERNtKC8SGti/pLNi+FNiwkUQRETEQinsxSfoH4LR6+SDg1mYiRUTEICieUQ7YDPgh8KP6dYb7jogYx0p7Mf0eOKrhLBERMUBGLRCSTrH9EUk/oRp76Rlsv72xZBER0ap+ZxCdNocvNB0kIiIGy6gFwvb8+uWOtv+p+z1Js4DLmgoWERHtKm2kfn+PdR8YwxwRETFg+rVBzADeA0yVdF7XWxuTOakjIsa1fm0QVwB3A5OAk7rWLwOubypURES0r18bxO3A7cDrVk+ciIgYFKWD9e0qaa6khyU9LmmFpIeaDhcREe0pbaT+EjAD+A3wXOB/Al9sKlRERLSvdCwmbC+SNMH2CuA7kq5oMFdERLSstEA8Kml94FpJn6NquM5orhER41jpJaa/AiYARwCPAFsB72oqVEREtK90sL7b65ePASc0FyciIgZFvxvlbqDHIH0dtncY80QRETEQ+p1B7FM/H14/dwbvey/waCOJIiJiIJTcKIek3Wzv1vXWcZIuBz7VZLiIiGhP8ZzUknbvLEh6PenFFBExrpV2cz0E+LakTerlB6imIY2IiHGqtBfTfODVkiYCsv1gs7EiIqJt/XoxHWT7e5KOHrYeANsnN5gtIiJa1K8NotPOsPEIj1FJ2lvSzZIWSTpulO12qgcA3G9l942IiGb068X09fp5pW+OkzQB+DLwZmAJMFfSebZv6rHdicCcld03IiKa0+8S0z+P9r7to0Z5e2dgke3F9WedCUwHhv+SPxI4B9hpFfaNiIiG9Guknv8sPnsycEfX8hJgl+4NJE0G9gXeyDMLRN99IyKiWf0uMZ36LD5bvT5y2PIpwLG2V3Qavldi32pD6VDgUICtt956FWJGREQvRd1cJW0GHAtsD2zQWW/7jaPstoRq1NeOLYG7hm0zBJxZF4dJwDRJywv37WSYDcwGGBoaGnHcqIiIWDmld1KfDiwEplKN5nobMLfPPnOB7SRNreeSeDdwXvcGtqfanmJ7CnA2cJjtc0v2jYiIZpUWiBfa/hbwhO3LbB8M7DraDraXU80fMYequJxle4GkmZJmrsq+hVkjImIMlA618UT9fLekt1Jd7tmy3062LwAuGLbuayNs+4F++0ZExOpTWiD+Tz0O098AXwQmAh9tLFVERLSutEBcWY+/9CCwZ4N5IiJiQJS2QVwh6WJJh0h6QaOJIiJiIBQVCNvbAR8HXgnMl3S+pIMaTRYREa0qPYPA9lW2j6YaBuN+4NncRBcREQOuqEBImijp/ZIuBK4A7qYqFBERMU6VNlJfB5wLfMr2rxrMExERA6K0QGxjO8NYRESsRUobqVMcIiLWMsWN1BERsXZJgYiIiJ5KezF9ru7JtJ6kn0u6L/dBRESMb6VnEHvZfgjYh2quhpcBxzSWKiIiWldaINarn6cBZ9i+v6E8ERExIEq7uf5E0q+Bx4DD6hnm/tBcrIiIaFtpN9fjgNcBQ7afAB4BpjcZLCIi2lXaSL0/sNz2CkkfB74HvLjRZBER0SqV3AMn6XrbO0jaHfgM8AXg72zv0nTAlSHpXuD2Hm9NAu5bzXGejTUp75qUFdasvMnanDUpb9NZX2J7s15vlLZBrKif3wp81faPJX1yLJKNpZF+SEnzbA+t7jyrak3KuyZlhTUrb7I2Z03K22bW0l5Md0r6OnAAcIGk56zEvhERsQYq/SV/ADAH2Nv2A8Cm5D6IiIhxrbQX06PALcBfSjoC2Nz2xY0mG1uz2w6wktakvGtSVliz8iZrc9akvK1lLW2kngX8NfDDetW+wGzbX2wwW0REtKi4FxPwOtuP1MsbAr+yvUPD+SIioiWlbRDi6Z5M1K819nHGnqS9Jd0saZGk49rOMxpJt0m6QdK1kua1nWc4Sd+WtFTSjV3rNpX0M0m/qZ9f0GbGjhGyflLSnfXxvVbStDYzdkjaStIlkhZKWlCfsQ/ysR0p78AdX0kbSLpK0nV11hPq9YN6bEfK28qxLT2DOBp4P/CjetU7gO/aPqXBbM+apAnAfwJvphpkcC4ww/ZNrQYbgaTbqO5WH8j+2ZLeADwM/IvtV9XrPgfcb/uzdQF+ge1j28xZ5+qV9ZPAw7a/0Ga24SRtAWxh+2pJGwPzqf4f+wCDeWxHynsAA3Z8JQnY0PbDktYD/h2YBbyTwTy2I+XdmxaObWkj9cnAB4H7gd8DHxz04lDbGVhke7Htx4EzyRAhq8z2L6j+DXSbDpxavz6V6hdF60bIOpBs32376vr1MmAhMJnBPbYj5R04rjxcL65XP8zgHtuR8raib4GQtI6kG21fbfufbf+T7WtWR7gxMBm4o2t5CQP6D7lm4GJJ8yUd2naYQi+yfTdUvziAzVvO088Rkq6vL0ENxGWFbpKmAK8BrmQNOLbD8sIAHl9JEyRdCywFfmZ7oI/tCHmhhWPbt0DYfhK4TtLWqyHPWOvVTjLI82vvZvu1wFuAw+vLJDF2vgpsC+wI3A2c1G6cZ5K0EXAO8JF6/pWB1iPvQB5f2yts7whsCews6VVtZxrNCHlbObaljdRbAAtUzSZ3XufRZLAxsgTYqmt5S+CulrL0Zfuu+nkpVXvPzu0mKnJPfU26c216act5RmT7nvp/vieBbzBAx7e+3nwOcLrtTnfygT22vfIO8vEFqG/yvZTqev7AHtuO7rxtHdvSAnEC1Wxyn6KqXJ3HoJsLbCdpqqT1gXcDA1nYJG1YN/h1uhHvBdw4+l4D4TyqDgzUzz9uMcuoOr8QavsyIMe3bpj8FrCwbu/rGMhjO1LeQTy+kjaT9Pz69XOBNwG/ZnCPbc+8bR3bUXsxSXop1bW6y4etfwNwp+1bGs73rNXdwU4BJgDftv3pliP1JGkbnu4lti7w/UHLKukMYA+q0SXvAY4HzgXOArYGfgvsPwgzDo6QdQ+qU3QDtwEf6lyHbpOqUZJ/CdwAPFmv/juq6/qDeGxHyjuDATu+knagaoSeQPUH8Vm2PyXphQzmsR0p72m0cGz7FYjzqYb1vn7Y+iHgeNtvazhfRES0pN8lpinDiwOA7XnAlEYSRUTEQOhXIDYY5b3njmWQiIgYLP0KxFxJfz18paRDqO6ejIiIcapfG8SLqBpOH+fpgjAErA/sa/u/Gk8YERGtKB2LaU+gc3PJAtv/1miqiIhoXelYTJfY/mL9SHGIEUmypJO6lj+mMZq/XNJ3Je03Fp/V53v2VzVS6SU93vt8Pcrm51fhc3cchBFORyPp4f5b9dzvHZK2X13fF6tH5pWOsfZH4J2SJrUdpFs9sm+pQ4DDbO/Z470PAa+1vSpT7u4IrFSBUGVN+P/0HcBKF4gYbGvCP7xYsyynmiLxo8PfGH4G0PnrUdIeki6TdJak/5T0WUnvVTUu/g2Stu36mDdJ+mW93T71/hPqv+zn1oOZfajrcy+R9H2qm7qG55lRf/6Nkk6s130C2B342vCzhHp4mQ2BKyUdWN/1ek79vXMl7VZvt7OkKyRdUz+/vL6T/1PAgarG8z9Q1Rj/H+v6/BslTakfCyV9Bbga2ErSXpJ+JelqST9QNQ4S9bG6qf65/2QoaEl/rqfnELim6279Y7qO1wm9/kOOtI2k99XrrpN0mqTXA28HPl9/z7b14yJVA0/+UtIr6n2n1j/HXEn/u9f3xgCxnUceY/agmoNhItXdnpsAHwM+Wb/3XWC/7m3r5z2AB6jG/HoOcCdwQv3eLOCUrv0vovrDZjuqsbY2AA4FPl5v8xxgHjC1/txHgKk9cr6Y6g7azajuXP834B31e5dSzcvR8+frev19YPf69dZUQ09Q//zr1q/fBJxTv/4A8KWu/T8JfKxr+Uaq+4umUN2hvGu9fhLwC6p5AgCOBT4BbArczNNtic/vkfcnVINAAmxU/6x7URVx1cfyfOANw/6b9NwGeGX9nZPq7TYd4b/tz4Ht6te7AP9Wvz4PeF/9+vDu45nH4D3WJWKM2X5I0r8ARwGPFe421/XQAZJuAS6u198AdF/qOcvVgGW/kbQYeAXVL7Mdus5ONqEqII8DV9m+tcf37QRcavve+jtPp/oFeG5hXqh++W8vPTVo8MT6L/RNgFMlbUc1NMJ6K/GZHbfb/o/69a5Ul28ur79rfeBXwEPAH4BvSvop1S/x4S4HTq5/vh/aXiJpL6pj1hm2fyOq4/WLrv1G2ubVwNmuJ7Vyj+Ep6rOb1wM/6Do2z6mfdwPeVb8+DTix75GI1qRARFNOobo88p2udcupL2uq+s2xftd7f+x6/WTX8pM889/p8G53pvor90jbc7rfkLQH1RlEL2MxZe46VHO1P6MISvoicIntfVXNl3DpCPs/dTxq3TemducW1bwAM4Z/gKSdgb+gGojyCOCN3e+7mjHtp1RtH/8h6U31533G9tdH+dl6biPpKPoPmb8O8ICrIat7GeQh96NL2iCiEfVflmdRNfh23Ab8Wf16Oqv2l/X+qiax2hbYhupyxxzgw6qGoEbSy1SNiDuaK4E/lzSpbsCeAVy2klkupvqlTP29nV+Im1BdJoPqslLHMmDjruXbgNfW+76W6rJYL/8B7KZq8EwkPa/+GTcCNrF9AfARqgMNdGcAAAFESURBVEbwZ5C0re0bbJ9IdentFVTH6+CudozJkoZPmDPSNj8HDlA12B2SNh3+s7maG+JWSfvX20jSq+vtLqcqZgDvHeHnjQGRAhFNOonq+nnHN6h+KV9FdV16pL/uR3Mz1S/yC4GZtv8AfBO4Cbha0o3A1+lzdlxfzvpb4BLgOuBq2ys75PNRwFDdYHsTMLNe/zngM5IupxqVs+MSqktS10o6kGo+hU1VzR72Yar503tlvZeq0Jwh6XqqgvEKql/I59frLqNHxwDgI3Xj93VUl/sutH0xVfvJryTdAJzNMwsXI21jewHwaeCy+jM7w32fCRxTN4RvS/XL/5B6mwU8PdXvLKrJsOZSFdIYYEU3ykVExNonZxAREdFTCkRERPSUAhERET2lQERERE8pEBER0VMKRERE9JQCERERPaVARERET/8faOFER9weXa8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Instead of reducing the feature dimensions using PCA, we will use try the recursive feature elimination in order to check\n",
    "#if we could improve our model and discover which featuresare hughly ranked with the best hyperparameters\n",
    "#It only works with the linear kernel so we will try it with the best values for linear found before \n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "estimator = svm.SVC(kernel='linear', C=1) \n",
    "svc_rfe = RFECV(estimator, step=1, cv=5, scoring = 'f1_macro')\n",
    "svc_rfe.fit(X_train_sc, y_train)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % svc_rfe.n_features_)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(svc_rfe.grid_scores_) + 1), svc_rfe.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see that recursive feature elimination does not improve our score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Criterion: gini\n",
      "Best max_depth: 12\n",
      "Best Number Of Components: 17\n",
      "Best Score: 0.57\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77        80\n",
      "           1       0.38      0.35      0.37        31\n",
      "\n",
      "    accuracy                           0.66       111\n",
      "   macro avg       0.57      0.56      0.57       111\n",
      "weighted avg       0.65      0.66      0.65       111\n",
      "\n",
      "Confusion Matrix: \n",
      " [[62 18]\n",
      " [20 11]]\n"
     ]
    }
   ],
   "source": [
    "#Decision Trees\n",
    "\n",
    "#Decision trees tend to overfit on data with a large number of features so first we will perform dimensionality reduction\n",
    "#using pca\n",
    "\n",
    "#The rationale is to take advantage of the Pipeline function of sklearn in order to grid search for the best number of PCA \n",
    "#components to input in our decision trees, and the best hyperparameters of our tree at the same time \n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "decisiontree = DecisionTreeClassifier()\n",
    "\n",
    "# Create a pipeline of two steps: \n",
    "# 1) tranform the data with PCA, 2) train a Decision Tree Classifier on the data.\n",
    "pipe = Pipeline(steps=[('pca', pca), ('decisiontree', decisiontree)])\n",
    "\n",
    "# Create Parameter Space\n",
    "\n",
    "# Create a list of a sequence of integers to integrate from PCA\n",
    "n_components = list(range(1,X_train_sc.shape[1]+1,1))\n",
    "\n",
    "# Create lists of parameter for Decision Tree Classifier\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [4,6,8,12]\n",
    "\n",
    "parameters = dict(pca__n_components=n_components,\n",
    "                      decisiontree__criterion=criterion,\n",
    "                      decisiontree__max_depth=max_depth)\n",
    "\n",
    "\n",
    "clf = GridSearchCV(pipe, parameters, cv = 10, scoring = 'f1_macro')\n",
    "\n",
    "clf.fit(X_train_sc, y_train)\n",
    "\n",
    "print('Best Criterion:', clf.best_estimator_.get_params()['decisiontree__criterion'])\n",
    "print('Best max_depth:', clf.best_estimator_.get_params()['decisiontree__max_depth'])\n",
    "print('Best Number Of Components:', clf.best_estimator_.get_params()['pca__n_components'])\n",
    "print(\"Best Score: %0.2f\" % clf.best_score_)\n",
    "print()\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test_sc)\n",
    "print(\"Classification Report:\", \"\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\", \"\\n\", confusion_matrix(y_true, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best leaf_size: 1\n",
      "Best weight function: uniform\n",
      "Best n_neighbors: 1\n",
      "Best Score: 0.5223561066678386\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77        80\n",
      "           1       0.38      0.35      0.37        31\n",
      "\n",
      "    accuracy                           0.66       111\n",
      "   macro avg       0.57      0.56      0.57       111\n",
      "weighted avg       0.65      0.66      0.65       111\n",
      "\n",
      "Confusion Matrix: \n",
      " [[62 18]\n",
      " [20 11]]\n"
     ]
    }
   ],
   "source": [
    "#Knn \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#define the model and parameters\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "parameters = {'n_neighbors':[1,3,5,7,9,11,13,15,17,19,21], #usually odd numbers\n",
    "              'leaf_size':[1,2,3,5],\n",
    "              'weights':['uniform', 'distance']}\n",
    "\n",
    "#Fit the model\n",
    "model_knn = GridSearchCV(knn, param_grid=parameters, cv = 5, scoring = 'f1_macro')\n",
    "model_knn.fit(X_train_sc,y_train)\n",
    "\n",
    "print('Best leaf_size:', model_knn.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best weight function:', model_knn.best_estimator_.get_params()['weights'])\n",
    "print('Best n_neighbors:', model_knn.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score:', model_knn.best_score_)\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test_sc)\n",
    "print(\"Classification Report:\", \"\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\", \"\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe repeat knn with pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: {'n_estimators': 1500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 20, 'bootstrap': False}\n",
      "Best Score: 0.46\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77        80\n",
      "           1       0.38      0.35      0.37        31\n",
      "\n",
      "    accuracy                           0.66       111\n",
      "   macro avg       0.57      0.56      0.57       111\n",
      "weighted avg       0.65      0.66      0.65       111\n",
      "\n",
      "Confusion Matrix: \n",
      " [[62 18]\n",
      " [20 11]]\n"
     ]
    }
   ],
   "source": [
    "#Random Forests\n",
    "#Let's tune the hyperparameters\n",
    "\n",
    "#Here we will try a different approach - we will use randomize grid at first to limit our search space and then we will use\n",
    "# a more exhaustive approach\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [200, 500, 1000, 1500, 2500]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [10, 20, 30, 40, 50]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15, 20]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, scoring = 'f1_macro')\n",
    "\n",
    "rf_random.fit(X_train_sc, y_train)\n",
    "\n",
    "print(\"Best Score:\", rf_random.best_params_)\n",
    "print(\"Best Score: %0.2f\" % rf_random.best_score_)\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test_sc)\n",
    "print(\"Classification Report:\", \"\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\", \"\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: {'bootstrap': True, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 1500}\n",
      "Best Score: 0.46\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77        80\n",
      "           1       0.38      0.35      0.37        31\n",
      "\n",
      "    accuracy                           0.66       111\n",
      "   macro avg       0.57      0.56      0.57       111\n",
      "weighted avg       0.65      0.66      0.65       111\n",
      "\n",
      "Confusion Matrix: \n",
      " [[62 18]\n",
      " [20 11]]\n"
     ]
    }
   ],
   "source": [
    "#Random Forests 2\n",
    "#Let's see if we can improve the score by grinding search around the best parameters\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#define the model and parameters\n",
    "rf_v2 = RandomForestClassifier()\n",
    "\n",
    "n_estimators = [1000, 1500, 2000]\n",
    "max_features = ['auto']\n",
    "max_depth = [10,20, 30, 40]\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "min_samples_leaf = [1, 2, 5]\n",
    "bootstrap = [True]\n",
    "\n",
    "parameters = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "#Fit the model\n",
    "model_rf_v2 = GridSearchCV(rf_v2, param_grid=parameters, cv = 5, scoring = 'f1_macro')\n",
    "model_rf_v2.fit(X_train_sc,y_train)\n",
    "\n",
    "print(\"Best Score:\", model_rf_v2.best_params_)\n",
    "print(\"Best Score: %0.2f\" % model_rf_v2.best_score_)\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test_sc)\n",
    "print(\"Classification Report:\", \"\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\", \"\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The score did not change whatsoever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance\n",
    "#https://machinelearningmastery.com/calculate-feature-importance-with-python/#:~:text=Feature%20importance%20refers%20to%20a,feature%20when%20making%20a%20prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will try:\n",
    "#SVM\n",
    "#General discriminant analysis\n",
    "#Decision Trees\n",
    "#Random Forest\n",
    "#GLMs (generalized linear models) -- logistic regression tried in paper\n",
    "#Ensembl of methods\n",
    "#linear model elastic net\n",
    "\n",
    "#do feature selection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection before SVM (PCA?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
